# DescripNN.py
import streamlit as st

def page_DescripNN():
    st.title('**Description Neural Network**')
    st.subheader('üìä Monetary Policy Impact ')
    st.markdown("---")

    st.subheader('ALL FEATUREs ')
    st.write("""**All features for predicting the impact of economic changes consist of 6 features**                                                  
                                        They are as follows:
    """)
    
    st.markdown("""
        <style>
            ul {
                padding-left: 50px;
            }
            li {
                font-size: 14px;
            }
        </style>
    """, unsafe_allow_html=True)

    st.markdown("""
        1. interest_rate_change  
        <ul>
            <li>Interest rate changes are a key factor in the economy, affecting investment and borrowing decisions.</li>
            <li>Higher interest rates can increase borrowing costs, affecting decisions on investment and home buying.</li>
            <li>This data is generated using <code>np.random.uniform(-1, 2, n_samples)</code></li>
            <li>Interest rate changes will range from -1 to 2 percent.</li>
        </ul>

        2. gdp_growth  
        <ul>
            <li>GDP growth is an important indicator of overall economic health, influencing investment and property prices.</li>
            <li>This data is generated using <code>np.random.uniform(-5, 5, n_samples)</code></li>
            <li>GDP growth will range from -5% to 5%.</li>
        </ul>

        3. inflation_rate  
        <ul>
            <li>Inflation reflects the risk of maintaining currency stability in the economy. If inflation is high, it may reduce the value of money.</li>
            <li>This data is generated using <code>np.random.uniform(0, 10, n_samples)</code></li>
            <li>Inflation rate will range from 0% to 10%.</li>
        </ul>

        4. unemployment_rate  
        <ul>
            <li>Unemployment rate directly impacts the economy and consumer purchasing power. A higher unemployment rate generally means less demand for housing.</li>
            <li>This data is generated using <code>np.random.uniform(0, 15, n_samples)</code></li>
            <li>Unemployment rate will range from 0% to 15%.</li>
        </ul>

        5. policy_announcement  
        <ul>
            <li>Economic policy announcements can impact the real estate sector, particularly in areas such as interest rate adjustments or support for borrowing.</li>
            <li>This data is generated using <code>np.random.choice([0, 1], size=n_samples)</code></li>
            <li>The policy announcement will be either 0 or 1, representing positive or negative policy impacts on the economy.</li>
        </ul>

        6. impact  
        <ul>
            The impact of the features mentioned above is calculated as a net impact value, showing the overall effect on house prices, calculated using the formula:
            <li><code>Impact = 0.5 * Interest_Rate_Change + 0.3 * GDP_Growth - 0.2 * Inflation_Rate - 0.1 * Unemployment_Rate + Noise</code></li>
            <li>Interest Rate Change (Interest_Rate_Change) has a weight of 0.5 in determining house prices.</li>
            <li>GDP Growth (GDP_Growth) has a weight of 0.3 in determining house prices.</li>
            <li>Inflation Rate (Inflation_Rate) has a negative weight of -0.2, impacting house prices negatively.</li>
            <li>Unemployment Rate (Unemployment_Rate) has a negative weight of -0.1, impacting house prices negatively.</li>
            <li>Noise is a random effect added to introduce variability in the results.</li>
        </ul>
    """, unsafe_allow_html=True)

    st.markdown("---")
    st.subheader('DATA source ')
    st.write("""
        This dataset is **generated by ChatGPT** for the Monetary Policy Impact model, consisting of features such as Interest Rate Change, GDP Growth, Inflation Rate, Unemployment Rate, Policy Announcement, and Impact. The model used is a **Regression Model** designed to predict the impact of monetary policy changes based on these features.
    """)

    #--------------------------------------------------------------------------------------------------------------------
    st.markdown("---")
    st.subheader('üß† Neural Network : NN')
    st.write("The selected model for developing Machine Learning is **Neural Network (NN)**")
    st.write("""
            It is a model inspired by the human brain, consisting of layers of nodes, also known as "neurons."
            <ul style="padding-left: 50px;">
                <li>It consists of multiple layers that can learn complex functions, starting from the Input layer which receives data and passing it to the Hidden layers for processing.</li>
                <li>Activation functions like ReLU or Sigmoid are used to modify the values in each neuron during calculations.</li>
                <li>Data is processed through several layers using mathematical functions, and the result is sent to the Output layer to produce the final result.</li>
                <li>Neural Network models can learn from complex data by adjusting weights to improve prediction accuracy.</li>
            </ul>
        """, unsafe_allow_html=True)
    #-----------------------------------------------------------------------------------------------------------------------

    st.markdown("---")
    st.subheader('üèπ Prepare DATA ')
    st.write("""**Main steps used for handling missing data and formatting data for model training** are as follows""")
    st.write("""
            1. Handling missing data: When certain data points are missing, such as empty fields or NaN, the following methods are used..
            <ul style="padding-left: 50px;">
                <li> <code>SimpleImputer</code> is a tool used to fill in missing values in the dataset.</li>
                <li> <code>strategy='mean'</code> is the method used to fill missing values by replacing them with the column's mean (for numerical data).</li>
                <li> <code>fit_transform(X)</code> calculates the mean of X data to fill missing values.</li>
                <li> <code>pd.DataFrame(imputer.fit_transform(X), columns=X.columns)</code> After filling in missing values, the data is converted back into a DataFrame with the original column names.</li>
            </ul>
        """, unsafe_allow_html=True)
    st.write("""
            2. Splitting data into train and test sets: This step divides the data into two parts: the Training Set for model training and the Test Set for model evaluation.
            <ul style="padding-left: 50px;">
                <li> <code>X_imputed</code> is the dataset with missing values filled in.</li>
                <li> <code>y</code> is the target variable.</li>
                <li> <code>test_size=0.2</code> splits the data into 80% for training and 20% for testing. <code>random_state=42</code> ensures that the results are reproducible.</li>
            </ul>
        """, unsafe_allow_html=True)
    st.write("""
            3. Feature Scaling: This step ensures that all features are standardized for model training.
            <ul style="padding-left: 50px;">
                <li> <code>StandardScaler</code> scales the data to have a mean of 0 and standard deviation of 1, making all variables more comparable.</li>
                <li> <code>fit_transform(X_train)</code> calculates the mean and standard deviation from the Training Set and scales the data accordingly.</li>
                <li> <code>transform(X_test)</code> applies the calculated scaling parameters to the Test Set.</li>
            </ul>
        """, unsafe_allow_html=True)
    
    #--------------------------------------------------------------------------------------------------------------------
    st.markdown("---")
    st.subheader('üèã STEPS in Model Development')
    st.write("""
        Model development steps:

        1. Data Preprocessing
        <ul style="padding-left: 50px;">
            <li>Handle missing values using <code>SimpleImputer</code>, which fills missing values with the mean of each column.</li>
            <li>Split data into training and testing sets using <code>train_test_split</code>.</li>
            <li>Scale the data using <code>StandardScaler</code> to ensure features have a mean of 0 and standard deviation of 1.</li>
        </ul>

        2. Model Creation üî®
        <ul style="padding-left: 50px;">
            <li>Choose a suitable algorithm, such as <strong>Neural Network (NN) ü§ñ</strong>.</li>
            <li>The NN model consists of multiple layers working together: Input Layer, Hidden Layers, and Output Layer.</li>
            <li>Use Activation functions like <code>ReLU</code> or <code>Sigmoid</code> in each layer to help the model learn from complex data.</li>
            <li>The model trains on the prepared dataset using Backpropagation to adjust weights and improve accuracy.</li>
            <li>Use <code>Sequential</code> from Keras to create the NN model and <code>Dense</code> Layer for each layer in the model.</li>
        </ul>

        3. Model Evaluation
        <ul style="padding-left: 50px;">
            <li>Use the Test Set to evaluate the model's performance.</li>
            <li>Assess the model using error metrics such as <code>Mean Absolute Error (MAE)</code> or <code>Accuracy</code> depending on the problem type (Regression or Classification).</li>
        </ul>

        4. Model Tuning
        <ul style="padding-left: 50px;">
            <li>Use <code>Grid Search üîç</code> or <code>Random Search</code> to find the best hyperparameters for the model.</li>
            <li>Tune the model's parameters such as the number of layers, units in each layer, and the learning rate to optimize performance.</li>
        </ul>

        5. Results Presentation
        <ul style="padding-left: 50px;">
            <li>Present the model's predictions along with error metrics and comparison graphs between actual and predicted values.</li>
            <li>Display the results in an easy-to-understand format, such as graphs or simple text explanations.</li>
        </ul>
    """, unsafe_allow_html=True)

    #----------------------------------------------------------------------------------------------------------------------

    st.markdown("---")
    st.subheader('üèóÔ∏è Model Architecture Design for the Dataset')
    st.write("""

        1. Input Layer
        <ul style="padding-left: 50px;">
            <li>The number of neurons in the input layer is determined by <code>input_dim=X_train.shape[1]</code>, which corresponds to the number of features in the dataset (Interest_Rate_Change, GDP_Growth, Inflation_Rate, Unemployment_Rate, Policy_Announcement).</li>
        </ul>

        2. Hidden Layers
        <ul style="padding-left: 50px;">
            <li>The first hidden layer consists of 256 neurons. The subsequent hidden layers contain 128 and 64 neurons. <code>BatchNormalization</code> is used in some layers to help speed up the learning process. <code>Dropout (0.3)</code> is applied to reduce overfitting.</li>
        </ul>

        3. Regularization (L2)
        <ul style="padding-left: 50px;">
            <li> L2 regularization is applied to all hidden layers to reduce the risk of overfitting.</li>
        </ul>

        4.  Output Layer
        <ul style="padding-left: 50px;">
            <li>The output layer has 1 neuron with no activation function, which is suitable for regression tasks where a numerical value is predicted.</li>
        </ul>

        5.  Optimizer (Adam)
        <ul style="padding-left: 50px;">
            <li>The <code>mean_squared_error (MSE)</code> loss function is used, which is appropriate for regression tasks</li>
        </ul>
             
        6.  Loss Function
        <ul style="padding-left: 50px;">
            <li>The Adam optimizer is used with a learning rate set to 0.001.</li>
        </ul>
             
        7.  Early Stopping
        <ul style="padding-left: 50px;">
            <li><code>EarlyStopping</code> is used to halt training when <code>val_loss</code> stops improving after a specified number of epochs (with patience=5).</li>
        </ul>

    """, unsafe_allow_html=True)





