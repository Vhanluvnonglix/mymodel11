# DescripML.py
import streamlit as st

def page_DescriptML():
    st.title('**Description Machine Learning**')
    st.subheader('üìä Predict House Price ')
    st.markdown("---")

    st.subheader('ALL FEATURES')
    st.write("""**The features used to predict house prices consist of 6 features:**  
    These are as follows:
    """)

    st.markdown("""
    <style>
        ul { 
            padding-left: 50px;
        }
        li {
            font-size: 14px;
        }
    </style>
    """, unsafe_allow_html=True)

    st.markdown("""
        1. square_footage  
        <ul>
            <li>The size of the house is directly related to the price of the house, with larger houses being more expensive than smaller ones.</li>
            <li>This data is generated using <code>np.random.uniform(500, 5000, n_samples)</code></li>
            <li>The size of the house ranges from 500 to 5000 square feet.</li>
        </ul>

        2. interest_rate  
        <ul>
            <li>The interest rate is a crucial factor affecting the cost of buying a house. If the interest rate rises, the cost of borrowing increases accordingly.</li>
            <li>This data is generated using <code>np.random.uniform(1, 10, n_samples)</code></li>
            <li>The interest rate ranges from 1% to 10%.</li>
        </ul>

        3. bedrooms  
        <ul>
            <li>The number of bedrooms is a feature that influences the house price, with houses having more bedrooms generally being more expensive.</li>
            <li>This data is generated using <code>np.random.randint(1, 6, n_samples)</code></li>
            <li>The number of bedrooms ranges from 1 to 5 rooms.</li>
        </ul>

        4. age_of_house  
        <ul>
            <li>The age of the house is another factor that affects the house price, with newer houses typically being more expensive than older ones.</li>
            <li>This data is generated using <code>np.random.randint(1, 100, n_samples)</code></li>
            <li>The age of the house ranges from 1 to 100 years.</li>
        </ul>

        5. people_per_household  
        <ul>
            <li>The number of people per household reflects the demand for housing, which can influence the house price.</li>
            <li>This data is generated using <code>np.random.uniform(1, 5, n_samples)</code></li>
            <li>The number of people per household ranges from 1 to 5.</li>
        </ul>

        6. price  
        <ul>
            The house price is calculated from the features mentioned above, combining values like:</li>
            <li>The house age</li>
            <li>The number of bedrooms</li>
            <li>The house size</li>
            <li>The interest rate</li>
            <li>The number of people per household</li>
        </ul>
    """, unsafe_allow_html=True)

    st.markdown("---")
    st.subheader('DATA source')
    st.write(""" 
        **This dataset is generated by ChatGPT**, containing factors such as house size, interest rates, number of bedrooms, house age, and other factors influencing house prices. The model used is a **Regression Model**.
    """)

#------------------------------------------------------------------------------------------------------------------------------- 
    st.markdown("---")
    st.subheader('üèπ Prepare DATA')
    st.write("""**The main steps to handle missing data and transform it into a suitable format for model training are as follows:**""")
    st.write("""
            1. Handling Missing Data: In cases where some data points are missing, such as empty cells or NaN values, we use:
            <ul style="padding-left: 50px;">
                <li> <code>SimpleImputer</code> is a tool used to fill in missing values in the dataset.</li>
                <li> <code>strategy='mean'</code> is the method used to fill missing values by replacing them with the mean of each column (for numerical data).</li>
                <li> <code>fit_transform(X)</code> calculates the mean of the data in <code>X</code> to replace the missing values.</li>
                <li> <code>pd.DataFrame(imputer.fit_transform(X), columns=X.columns)</code> converts the data back into a DataFrame with the original column names after filling the missing values.</li>
            </ul>
        """, unsafe_allow_html=True)

    st.write("""
            2. Splitting Data into Training and Test Sets: This step divides the data into two parts: one for training the model (Training Set) and one for testing the model (Test Set) to evaluate the model's performance.
            <ul style="padding-left: 50px;">
                <li> <code>X_imputed</code> is the data after filling the missing values.</li>
                <li> <code>y</code> is the target variable.</li>
                <li> <code>test_size=0.2</code> divides the data into 80% for training and 20% for testing, while <code>random_state=42</code> ensures reproducibility.</li>
            </ul>
        """, unsafe_allow_html=True)

    st.write("""
            3. Feature Scaling: This step ensures that the data is scaled appropriately for training and testing the model.
            <ul style="padding-left: 50px;">
                <li> <code>StandardScaler</code> is used to scale the data to have a mean of 0 and standard deviation of 1, which makes the data variables more comparable.</li>
                <li> <code>fit_transform(X_train)</code> calculates the mean and standard deviation of the training data and scales it accordingly.</li>
                <li> <code>transform(X_test)</code> applies the scaling to the test data using the parameters derived from the training data.</li>
            </ul>
        """, unsafe_allow_html=True)

#--------------------------------------------------------------------------------------------------------------------
    st.markdown("---")
    st.subheader('Random Forest & Gradient Boosting')
    st.write("The models selected for developing the Machine Learning system are **Random Forest** and **Gradient Boosting**")

    st.subheader('üå≤ Random Forest')
    st.write("""
        This algorithm is an extension of Decision Trees.
        <ul style="padding-left: 50px;">
            <li> It uses Bagging (Bootstrap Aggregating) to combine multiple trees together.</li>
            <li> It reduces the overfitting problem of Decision Trees by averaging the results of several trees.</li>
            <li> It is suitable for tasks requiring high accuracy and model variance reduction.</li>
        </ul>
    """, unsafe_allow_html=True)

    st.subheader('üöÄ Gradient Boosting')
    st.write("""
        This algorithm uses the concept of Boosting.
        <ul style="padding-left: 50px;">
            <li> Each tree is built to correct the errors of previous trees.</li>
            <li> It uses Gradient Descent to improve predictions iteratively.</li>
            <li> The resulting model is typically highly accurate, though it requires more time for training.</li>
        </ul>
    """, unsafe_allow_html=True)

#-------------------------------------------------------------------------------------------------------------------------------
    st.markdown("---")
    st.subheader('üèã STEPS in Model Development')

    st.write("""
        **1. Data Preprocessing (Common for Both Models)**
        <ul style="padding-left: 50px;">
            <li><strong>Handling Missing Values:</strong> Use <code>SimpleImputer</code> to fill in missing values in the data by replacing them with the mean of each column.</li>
            <li><strong>Splitting Data into Training and Test Sets:</strong> Use <code>train_test_split</code> to split the data into Training Set and Test Set with a test size of 0.2 (80% for Training, 20% for Test).</li>
            <li><strong>Scaling Data:</strong> Use <code>StandardScaler</code> to scale the data so that it has a mean of 0 and a standard deviation of 1 for model training.</li>
        </ul>

        **2. Model Creation üî®**
            <ul style="padding-left: 50px;">
                <li><strong>For Random Forest Model:</strong></li>
                    Create a <code>RandomForestRegressor</code> model and set parameters such as <code>n_estimators=200</code>, <code>max_depth=10</code>, <code>min_samples_split=10</code>.
                    Train the model with the prepared data using <code>model.fit(X_train, y_train)</code>.
                <li><strong>For Gradient Boosting Model:</strong></li>
                    Create a <code>GradientBoostingRegressor</code> model and use <code>RandomizedSearchCV</code> to find the best parameters like <code>n_estimators</code>, <code>learning_rate</code>, <code>max_depth</code>, <code>min_samples_split</code>, and <code>min_samples_leaf</code>.
                    Train the model with the data using <code>random_search.fit(X_train, y_train)</code>.
            </ul>

        **3. Model Evaluation (Common for Both Models)**
        <ul style="padding-left: 50px;">
            <li><strong>Evaluation Using Test Set:</strong> Use the Test Set from the data preprocessing step to evaluate the performance of the model.</li>
            <li><strong>Evaluation Metrics:</strong> Use Mean Absolute Error (MAE) or Mean Squared Error (MSE) to evaluate the model's performance.</li>
        </ul>

        
        **4. Model Tuning**
            <ul style="padding-left: 50px;">
                <li><strong>For Random Forest Model:</strong></li>
                If needed, you can use <code>Grid Search</code> or <code>Random Search</code> to find the best parameters such as <code>n_estimators</code>, <code>max_depth</code>.
                <li><strong>For Gradient Boosting Model:</strong></li>
                Use Random Search via <code>RandomizedSearchCV</code> to find the best parameters like <code>n_estimators</code>, <code>learning_rate</code>, <code>max_depth</code>, and others related.
            </ul>

        **5. Results Presentation**
        <ul style="padding-left: 50px;">
            <li><strong>Presentation for Both Models:</strong> Present the model results by comparing the Actual and Predicted values.</li>
            <li>Use graphs or text that are easy to understand, such as Error Metrics and graphs comparing True vs. Predicted Values, to clearly show the differences between actual and predicted values.</li>
        </ul>
    """, unsafe_allow_html=True)
